{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828a714",
   "metadata": {
    "id": "1828a714"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from nltk import word_tokenize\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3619ee5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3619ee5",
    "outputId": "533c1620-5f35-4d1f-d2fe-865fab10d805"
   },
   "outputs": [],
   "source": [
    "#Uncomment if running on Google Colab\n",
    "#!pip install pyconll\n",
    "import pyconll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d9f27",
   "metadata": {},
   "source": [
    "***\n",
    "Help functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29365991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenized_sentences, token_vocab, label_vocab, max_sentence_len, \n",
    "              max_token_len, tok_format=None):\n",
    "    #Vectorize sentences. \n",
    "    #tokenized_sentences: word tokens\n",
    "    #add one column before and after token to mark beginning and end of the token\n",
    "    data_tensor = torch.zeros((len(tokenized_sentences), max_sentence_len, max_token_len+2), \n",
    "                              dtype=torch.long)\n",
    "    if tok_format == 'pyconll':\n",
    "        labels_tensor = torch.zeros((len(tokenized_sentences), max_sentence_len), dtype=torch.long)\n",
    "\n",
    "    for i, sent in enumerate(tokenized_sentences):\n",
    "        for j, tok in enumerate(sent[:max_sentence_len]):\n",
    "            if tok_format == 'pyconll':\n",
    "                tok = tok.form\n",
    "            for k, ch in enumerate(tok.lower()[:max_token_len]):\n",
    "                data_tensor[i, j, k+1] = token_vocab.get(ch, 0)\n",
    "            if tok_format == 'pyconll':\n",
    "                labels_tensor[i, j] = label_vocab.get(tok.upos, label_vocab['X'])\n",
    "    if tok_format == 'pyconll':\n",
    "        return data_tensor, labels_tensor\n",
    "    else:\n",
    "        return data_tensor\n",
    "\n",
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                test_dataset, \n",
    "                loss_fun=nn.functional.cross_entropy,\n",
    "                lr=5e-3,\n",
    "                num_epoch=10,\n",
    "                batch_sz=64, \n",
    "                dev='cuda'):\n",
    "    \n",
    "    device = dev if torch.cuda.is_available() else 'cpu'\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, verbose=True)    \n",
    "    data_loader_train = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "    data_loader_val = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                 batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    for epoch in range(num_epoch):\n",
    "        st = time.perf_counter()\n",
    "        model.train()\n",
    "        for samples, labels in data_loader_train:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "            pred = model.forward(samples)\n",
    "            loss_val = loss_fun(pred, labels)\n",
    "            model.zero_grad()\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        model.eval()    \n",
    "        with torch.no_grad():\n",
    "            nb = 0\n",
    "            mean_loss = 0\n",
    "            for samples, labels in data_loader_val:\n",
    "                nb += 1\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                pred = model.forward(samples)\n",
    "                mean_loss += float(loss_fun(pred, labels))\n",
    "            mean_loss = mean_loss/nb\n",
    "            print(f\"Epoch {epoch} loss {mean_loss}, time {time.perf_counter()-st}.\")\n",
    "            if mean_loss < best_loss:\n",
    "                #best_model = copy.deep_copy(model)\n",
    "                best_model = copy.deepcopy(model)  #version for colab\n",
    "        scheduler.step(mean_loss)\n",
    "    return best_loss, best_model\n",
    "    \n",
    "def predict(model, dataset, predict_limit=None, batch_sz=64):\n",
    "    import tqdm\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if len(dataset) < batch_sz:\n",
    "        batch_sz = len(dataset)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset,batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    out_labels = []\n",
    "    predicted = []\n",
    "    with torch.no_grad():\n",
    "        num_predict = 0\n",
    "        for samples, labels in tqdm.tqdm(data_loader, total=len(dataset)/batch_sz ):\n",
    "            out_labels.append(labels.numpy())\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "            pred = model.forward(samples)\n",
    "            predicted.append(pred.detach().cpu().numpy())\n",
    "            num_predict += len(samples)\n",
    "            if predict_limit and num_predict >= predict_limit:\n",
    "                break\n",
    "    #print(f\"shape labels {np.array(out_labels).shape}, pred {np.array(predicted).shape}\")\n",
    "    return np.concatenate(out_labels), np.concatenate(predicted)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b4564",
   "metadata": {
    "id": "ed3b4564"
   },
   "source": [
    "***\n",
    "<font size=5>\n",
    "Get the train and test data. Tokenize it into character tokens. Vectorize it.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243bcec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "5243bcec",
    "outputId": "4877d1b1-f165-4183-b41d-8ed72799bca0"
   },
   "outputs": [],
   "source": [
    "'''import wget\n",
    "out = './data/ru_syntagrus-ud-train.conllu'\n",
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\"\n",
    "wget.download(url, out)\n",
    "out = './data/ru_syntagrus-ud-test.conllu'\n",
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\"\n",
    "wget.download(url, out)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd6bfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71cd6bfc",
    "outputId": "c4e850a1-8a18-49c5-d96c-404178b47851"
   },
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "model_dir = './models/'\n",
    "#If run on Google Colab uncomment\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#data_dir = '/content/drive/MyDrive/nlp_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce304191",
   "metadata": {
    "id": "ce304191"
   },
   "outputs": [],
   "source": [
    "train_data = pyconll.load_from_file(data_dir+'ru_syntagrus-ud-train.conllu')\n",
    "test_data = pyconll.load_from_file(data_dir+'ru_syntagrus-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69d4f197",
    "outputId": "6caa1b34-8846-4854-f2ec-49c7c28a4835"
   },
   "outputs": [],
   "source": [
    "print(' '.join([tok.form for tok in train_data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44621786",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44621786",
    "outputId": "cb2e64f6-0158-4c34-a86e-69ee143627cb"
   },
   "outputs": [],
   "source": [
    "MAX_TOKEN_LEN = max(len(tok.form) for sent in train_data for tok in sent)\n",
    "MAX_SENT_LEN = max(len(sent) for sent in train_data)\n",
    "print(f\"The longest sentence has {MAX_SENT_LEN} tokens\")\n",
    "print(f\"The longest token has {MAX_TOKEN_LEN} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea65f04",
   "metadata": {
    "id": "6ea65f04"
   },
   "outputs": [],
   "source": [
    "train_texts = [' '.join(tok.form for tok in sent).lower() for sent in train_data]\n",
    "test_texts = [' '.join(tok.form for tok in sent).lower() for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1aa81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cd1aa81",
    "outputId": "a6eee32c-fe1e-44e8-8a06-cb323cc0266f"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=False, analyzer = 'char')\n",
    "vect.fit_transform(train_texts)\n",
    "#Insert pad word into vocabulary, it's more convenient if pad word has value 0\n",
    "#hence swap it with whatever token has value 0\n",
    "last_keyval = len(vect.vocabulary_)\n",
    "for zerok, v in vect.vocabulary_.items():\n",
    "    if v == 0:\n",
    "        break\n",
    "vect.vocabulary_['<PAD>'] = 0\n",
    "vect.vocabulary_[zerok] = last_keyval\n",
    "print(f\"Vocabulary has {len(vect.vocabulary_)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac43d06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ac43d06",
    "outputId": "51a473cd-feb0-4de3-f867-fb1fed110a8d"
   },
   "outputs": [],
   "source": [
    "TAGS = sorted({token.upos for sent in train_data for token in sent if token.upos})\n",
    "#move \"tag unknown\"('X') to to front so that it has zero id\n",
    "TAGS = [TAGS[-1]] + TAGS[:-1]\n",
    "label2id = {label:id for id, label in enumerate(TAGS)}\n",
    "print(f\"There are total of {len(TAGS)} unique tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0469c",
   "metadata": {
    "id": "37b0469c"
   },
   "outputs": [],
   "source": [
    "train_tensor, train_labels_tensor = vectorize(train_data, \n",
    "                                              vect.vocabulary_, \n",
    "                                              label2id, \n",
    "                                              MAX_SENT_LEN, \n",
    "                                              MAX_TOKEN_LEN, \n",
    "                                              tok_format='pyconll')\n",
    "test_tensor, test_labels_tensor = vectorize(test_data, \n",
    "                                            vect.vocabulary_, \n",
    "                                            label2id, \n",
    "                                            MAX_SENT_LEN, \n",
    "                                            MAX_TOKEN_LEN, \n",
    "                                            tok_format='pyconll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db6f5a",
   "metadata": {
    "id": "a7db6f5a"
   },
   "outputs": [],
   "source": [
    "#Pack it into a dataset so that we can feed it in batches to the model\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468d0ff",
   "metadata": {
    "id": "f468d0ff"
   },
   "source": [
    "***\n",
    "<font size=5>\n",
    "    Model architecture\n",
    "</font>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61fe70",
   "metadata": {
    "id": "7a61fe70"
   },
   "outputs": [],
   "source": [
    "#A stack of 1D convolution layers\n",
    "class StackedConv1d(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_layers=1, kernel_size=3, dropout_probab=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Sequential(nn.Conv1d(num_features, num_features, kernel_size, padding=kernel_size//2), \n",
    "                                        nn.Dropout(dropout_probab), \n",
    "                                        nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x\n",
    "\n",
    "#POS tagger net that predicts POS of separate tokens without considering context\n",
    "class TokenPOSTaggerNet(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(emb_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(emb_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        batch_sz, sent_len, token_len = tokens.shape\n",
    "        #Collapse it into 2D Matrix (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN so that we could \n",
    "        #feed it into embeddings\n",
    "        flat_view = tokens.view(batch_sz*sent_len, token_len)\n",
    "        \n",
    "        #Get initial char embeddings (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN X EmbSize\n",
    "        emb = self.char_embeddings(flat_view)\n",
    "        #To pass it into neural network the order of dimentions should be: \n",
    "        #         NUM_SAMPLES X NUM_FEATURES X ...(other dimensions)...\n",
    "        #   Hence we need to change the dim order in data to \n",
    "        #   (BatchSize * MAX_SENT_LEN) x  EmbSize x MAX_TOKEN_LEN\n",
    "        emb = emb.permute(0, 2, 1)\n",
    "        \n",
    "        #Pass it through the convolution layers\n",
    "        features = self.backbone(emb) \n",
    "        #Use Max Pooling to transform character embeddings of a token into a token embedding\n",
    "        token_features = self.global_pooling(features).squeeze(-1) #(BatchSize * MAX_SENT_LEN) x  EmbSize\n",
    "        \n",
    "        #predict token labels\n",
    "        pred = self.out(token_features) #(BatchSize * MAX_SENT_LEN) x  NumLabels\n",
    "        #reshape it back into sentences\n",
    "        pred = pred.view(batch_sz, sent_len, self.num_labels)\n",
    "        #transpose the output so that the dimensions correspond to what is expected \n",
    "        #in the loss function\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        return pred\n",
    "    \n",
    "#POS tagger net that predicts POS of tokens considering context\n",
    "class ContextPOSTaggerNet(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.token_backbone = StackedConv1d(emb_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.context_backbone = StackedConv1d(emb_size, **kwargs)\n",
    "        self.out = nn.Linear(emb_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        batch_sz, sent_len, token_len = tokens.shape\n",
    "        #Collapse it into 2D Matrix (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN so that we could \n",
    "        #feed it into embeddings\n",
    "        flat_view = tokens.view(batch_sz*sent_len, token_len)\n",
    "        \n",
    "        #Get initial char embeddings (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN X EmbSize\n",
    "        emb = self.char_embeddings(flat_view)\n",
    "        #To pass it into neural network the order of dimentions should be: \n",
    "        #         NUM_SAMPLES X NUM_FEATURES X ...(other dimensions)...\n",
    "        #   Hence we need to change the dim order in data to \n",
    "        #   (BatchSize * MAX_SENT_LEN) x  EmbSize x MAX_TOKEN_LEN\n",
    "        emb = emb.permute(0, 2, 1)\n",
    "        \n",
    "        #Pass it through the convolution layers\n",
    "        features = self.token_backbone(emb) \n",
    "        #Use Max Pooling to transform character embeddings of a token into a token embedding\n",
    "        features = self.global_pooling(features).squeeze(-1) #(BatchSize * MAX_SENT_LEN) x  EmbSize (x 1)\n",
    "        \n",
    "        #Get context features\n",
    "        features = features.view(batch_sz, sent_len, self.emb_size).permute(0, 2, 1)\n",
    "        features = self.context_backbone(features) # BatchSize x EmbSize x MaxSentenceLen\n",
    "        features = features.permute(0,2,1).view(batch_sz*sent_len, self.emb_size)\n",
    "        #predict token labels\n",
    "        pred = self.out(features) # (BatchSize*MaxSentenceLen) x NumLabels \n",
    "        #Reshape it back\n",
    "        pred = pred.view(batch_sz, sent_len, self.num_labels).permute(0,2,1)\n",
    "\n",
    "        return pred\n",
    "#Class that tags POS\n",
    "class POSTagger:\n",
    "    def __init__(self, model, char2id, tags, max_sentence_len, max_token_len):\n",
    "        self.model = model\n",
    "        self.char2id = char2id\n",
    "        self.tags = tags\n",
    "        self.max_sent_len = max_sentence_len\n",
    "        self.max_tok_len = max_token_len\n",
    "        \n",
    "    def __call__(self, tokenized_sentences):\n",
    "        #tokenized_sentences: word tokens\n",
    "        char_tokenized = vectorize(tokenized_sentences, self.char2id, None, self.max_sent_len, \n",
    "                                  self.max_tok_len)\n",
    "        dataset = torch.utils.data.TensorDataset(char_tokenized, \n",
    "                                                 torch.zeros(len(tokenized_sentences), self.max_sent_len))\n",
    "        dummy_labels, pred = predict(self.model, dataset) #num_sent x num_labels x max_sent_len\n",
    "        pred = pred.argmax(1)\n",
    "        print(\"shape of pred\", pred.shape)\n",
    "        \n",
    "        out_tags = []\n",
    "        for i, sent in enumerate(tokenized_sentences): \n",
    "            out_tags.append([self.tags[label] for label in pred[i, :len(sent)]])\n",
    "        return out_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06cd8bb",
   "metadata": {
    "id": "f06cd8bb"
   },
   "source": [
    "***\n",
    "First train a model that tags separate tokens without considering the sentence context.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OjPkU7xdg8A3",
   "metadata": {
    "id": "OjPkU7xdg8A3"
   },
   "outputs": [],
   "source": [
    "emb_sz = 64\n",
    "dropout_p=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tagger_model = TokenPOSTaggerNet(len(vect.vocabulary_), len(label2id), num_layers=3, emb_size=emb_sz, dropout_probab=dropout_p)\n",
    "print('Number of params in the model: ', sum(np.product(t.shape) for t in token_tagger_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93bd03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb93bd03",
    "outputId": "574f4018-653e-4cbe-9f6b-b5d0ad04a69b"
   },
   "outputs": [],
   "source": [
    "#loss, token_tagger_model = train_model(token_tagger_model, train_dataset, test_dataset, \n",
    "                               #       num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f113ff",
   "metadata": {
    "id": "17f113ff"
   },
   "outputs": [],
   "source": [
    "#torch.save(token_tagger_model.state_dict(), model_dir+'token_pos-'+'emb'+str(emb_sz)+'-p'+str(dropout_p)+'.pth')\n",
    "token_tagger_model.load_state_dict(torch.load(model_dir+'token_pos-'+'emb'+str(emb_sz)+'-p'+str(dropout_p)+'.pth', map_location=torch.device('cpu'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38c741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c38c741",
    "outputId": "f83e23c5-91b6-44f8-fc1c-4ff07e8174f4"
   },
   "outputs": [],
   "source": [
    "labels, predicted_labels = predict(token_tagger_model, test_dataset)\n",
    "token_loss = nn.functional.cross_entropy(torch.tensor(predicted_labels), torch.tensor(labels))\n",
    "print(f\"Loss on test data {token_loss}\")\n",
    "print(classification_report(labels.reshape(-1), predicted_labels.argmax(1).reshape(-1), \n",
    "                      labels=list(label2id.values()), \n",
    "                      target_names = list(label2id.keys()), \n",
    "                      zero_division=0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585678e",
   "metadata": {
    "id": "0585678e"
   },
   "source": [
    "***\n",
    "Now train a model that tags tokens considering the sentence context.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b5f07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7b5f07",
    "outputId": "238330d1-7b53-4a94-bef1-bbb25c705b3e"
   },
   "outputs": [],
   "source": [
    "context_tagger_model = ContextPOSTaggerNet(len(vect.vocabulary_), len(label2id), num_layers=3, emb_size=emb_sz, dropout_probab=dropout_p)\n",
    "print('Number of params in the model: ', sum(np.product(t.shape) for t in context_tagger_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_loss, context_tagger_model = train_model(context_tagger_model, train_dataset, test_dataset, \n",
    "                                      num_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d48849",
   "metadata": {
    "id": "38d48849"
   },
   "outputs": [],
   "source": [
    "#torch.save(context_tagger_model.state_dict(), model_dir+'context_pos-'+'emb'+str(emb_sz)+'-p'+str(dropout_p)+'.pth')\n",
    "context_tagger_model.load_state_dict(torch.load(model_dir+'context_pos-'+'emb'+str(emb_sz)+'-p'+str(dropout_p)+'.pth', map_location=torch.device('cpu')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38056e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f38056e2",
    "outputId": "8794a063-d241-4382-e1be-103ab2668671"
   },
   "outputs": [],
   "source": [
    "labels, predicted_labels = predict(context_tagger_model, test_dataset)\n",
    "context_loss = nn.functional.cross_entropy(torch.tensor(predicted_labels), torch.tensor(labels))\n",
    "print(f\"Loss on test data {token_loss}\")\n",
    "print(classification_report(labels.reshape(-1), predicted_labels.argmax(1).reshape(-1), \n",
    "                      labels=list(label2id.values()), \n",
    "                      target_names = list(label2id.keys()), \n",
    "                      zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67068a7d",
   "metadata": {},
   "source": [
    "***\n",
    "Test the taggers on previously unseen data.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcacbc09",
   "metadata": {
    "id": "bcacbc09"
   },
   "outputs": [],
   "source": [
    "test_corpus = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_corpus_tokens = [word_tokenize(sent) for sent in test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70266f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokTagger = POSTagger(token_tagger_model, vect.vocabulary_, TAGS, MAX_SENT_LEN, MAX_TOKEN_LEN)\n",
    "tok_tags = tokTagger(test_corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bca20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(test_corpus_tokens):\n",
    "    print([(tok, tag) for tok, tag in zip(sent, tok_tags[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f94aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextTagger = POSTagger(context_tagger_model, vect.vocabulary_, TAGS, MAX_SENT_LEN, MAX_TOKEN_LEN)\n",
    "context_tags = contextTagger(test_corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(test_corpus_tokens):\n",
    "    print([(tok, tag) for tok, tag in zip(sent, context_tags[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e012ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging_with_CNN.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "nlpenv2",
   "language": "python",
   "name": "nlpenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
