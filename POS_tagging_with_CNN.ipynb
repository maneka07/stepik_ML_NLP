{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import pyconll\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b4564",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=5>\n",
    "Get the train and test data. Tokenize it into character tokens. Vectorize it.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import wget\n",
    "out = './data/ru_syntagrus-ud-train.conllu'\n",
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\"\n",
    "wget.download(url, out)\n",
    "out = './data/ru_syntagrus-ud-test.conllu'\n",
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\"\n",
    "wget.download(url, out)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e77037",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce304191",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pyconll.load_from_file(data_dir+'ru_syntagrus-ud-train.conllu')\n",
    "test_data = pyconll.load_from_file(data_dir+'ru_syntagrus-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([tok.form for tok in train_data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44621786",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_LEN = max(len(tok.form) for sent in train_data for tok in sent)\n",
    "MAX_SENT_LEN = max(len(sent) for sent in train_data)\n",
    "print(f\"The longest sentence has {MAX_SENT_LEN} tokens\")\n",
    "print(f\"The longest token has {MAX_TOKEN_LEN} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea65f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [' '.join(tok.form for tok in sent).lower() for sent in train_data]\n",
    "test_texts = [' '.join(tok.form for tok in sent).lower() for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=False, analyzer = 'char')\n",
    "vect.fit_transform(train_texts)\n",
    "#Insert pad word into vocabulary, it's more convenient if pad word has value 0\n",
    "#hence swap it with whatever token has value 0\n",
    "last_keyval = len(vect.vocabulary_)\n",
    "for zerok, v in vect.vocabulary_.items():\n",
    "    if v == 0:\n",
    "        break\n",
    "vect.vocabulary_['<PAD>'] = 0\n",
    "vect.vocabulary_[zerok] = last_keyval\n",
    "print(f\"Vocabulary has {len(vect.vocabulary_)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac43d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = sorted({token.upos for sent in train_data for token in sent if token.upos})\n",
    "#move \"tag unknown\"('X') to to front so that it has zero id\n",
    "TAGS = [TAGS[-1]] + TAGS[:-1]\n",
    "label2id = {label:id for id, label in enumerate(TAGS)}\n",
    "print(f\"There are total of {len(TAGS)} unique tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, token_vocab, label_vocab, max_sentence_len, max_token_len):\n",
    "    #Vectorize sentences\n",
    "    #add one column before and after token to mark beginning and end of the token\n",
    "    data_tensor = torch.zeros((len(data), max_sentence_len, max_token_len+2), dtype=torch.long)\n",
    "    labels_tensor = torch.zeros((len(data), max_sentence_len), dtype=torch.long)\n",
    "\n",
    "    for i, sent in enumerate(data):\n",
    "        for j, tok in enumerate(sent[:max_sentence_len]):\n",
    "            for k, ch in enumerate(tok.form.lower()[:max_token_len]):\n",
    "                data_tensor[i, j, k+1] = token_vocab.get(ch, 0)\n",
    "            labels_tensor[i, j] = label_vocab.get(tok.upos, label_vocab['X'])\n",
    "    return data_tensor, labels_tensor\n",
    "\n",
    "train_tensor, train_labels_tensor = vectorize(train_data, \n",
    "                                              vect.vocabulary_, \n",
    "                                              label2id, \n",
    "                                              MAX_SENT_LEN, \n",
    "                                              MAX_TOKEN_LEN)\n",
    "test_tensor, test_labels_tensor = vectorize(test_data, \n",
    "                                            vect.vocabulary_, \n",
    "                                            label2id, \n",
    "                                            MAX_SENT_LEN, \n",
    "                                            MAX_TOKEN_LEN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pack it into a dataset so that we can feed it in batches to the model\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468d0ff",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=5>\n",
    "    Model architecture\n",
    "</font>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee672147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A stack of 1D convolution layers\n",
    "class StackedConv1d(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_layers=1, kernel_size=3, dropout_probab=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Sequential(nn.Conv1d(num_features, num_features, kernel_size, padding=kernel_size//2), \n",
    "                                        nn.Dropout(dropout_probab), \n",
    "                                        nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x\n",
    "\n",
    "#POS tagger net that predicts POS of separate tokens without considering context\n",
    "class TokenPOSTaggerNet(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(emb_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(emb_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        batch_sz, sent_len, token_len = tokens.shape\n",
    "        #Collapse it into 2D Matrix (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN so that we could \n",
    "        #feed it into embeddings\n",
    "        flat_view = tokens.view(batch_sz*sent_len, token_len)\n",
    "        \n",
    "        #Get initial char embeddings (BatchSize * MAX_SENT_LEN) x  MAX_TOKEN_LEN X EmbSize\n",
    "        emb = self.char_embeddings(flat_view)\n",
    "        #To pass it into neural network the order of dimentions should be: \n",
    "        #         NUM_SAMPLES X NUM_FEATURES X ...(other dimensions)...\n",
    "        #   Hence we need to change the dim order in data to \n",
    "        #   (BatchSize * MAX_SENT_LEN) x  EmbSize x MAX_TOKEN_LEN\n",
    "        emb = emb.permute(0, 2, 1)\n",
    "        \n",
    "        #Pass it through the convolution layers\n",
    "        features = self.backbone(emb) \n",
    "        #Use Max Pooling to transform character embeddings of a token into a token embedding\n",
    "        token_features = self.global_pooling(features).squeeze(-1) #(BatchSize * MAX_SENT_LEN) x  EmbSize\n",
    "        \n",
    "        #predict token labels\n",
    "        pred = self.out(token_features) #(BatchSize * MAX_SENT_LEN) x  NumLabels\n",
    "        #reshape it back into sentences\n",
    "        pred = pred.view(batch_sz, sent_len, self.num_labels)\n",
    "        #transpose the output so that the dimensions correspond to what is expected \n",
    "        #in the loss function\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1924ba3",
   "metadata": {},
   "source": [
    "***\n",
    "Train the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_dataset, \n",
    "                test_dataset, \n",
    "                loss_fun=nn.functional.cross_entropy,\n",
    "                lr=5e-3,\n",
    "                num_epoch=10,\n",
    "                batch_sz=64, \n",
    "                dev='cuda'):\n",
    "    \n",
    "    device = dev if torch.cuda.is_available() else 'cpu'\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, verbose=True)    \n",
    "    data_loader_train = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "    data_loader_val = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                 batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    for epoch in range(num_epoch):\n",
    "        st = time.perf_counter()\n",
    "        model.train()\n",
    "        for samples, labels in data_loader_train:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "            pred = model.forward(samples)\n",
    "            loss_val = loss_fun(pred, labels)\n",
    "            model.zero_grad()\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        model.eval()    \n",
    "        with torch.no_grad():\n",
    "            nb = 0\n",
    "            mean_loss = 0\n",
    "            for samples, labels in data_loader_val:\n",
    "                nb += 1\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "                pred = model.forward(samples)\n",
    "                mean_loss += float(loss_fun(pred, labels))\n",
    "            mean_loss = mean_loss/nb\n",
    "            print(f\"Epoch {epoch} loss {mean_loss}, time {time.perf_counter()-st}.\")\n",
    "            if mean_loss < best_loss:\n",
    "              best_model = copy.deep_copy(model)\n",
    "        scheduler.step(mean_loss)\n",
    "    return best_loss, best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tagger = TokenPOSTaggerNet(len(vect.vocabulary_), len(label2id), num_layers=3)\n",
    "print('Number of params in the model: ', sum(np.product(t.shape) for t in best_tagger.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b53609",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_tagger = train_model(best_tagger, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0609e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv2",
   "language": "python",
   "name": "nlpenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
