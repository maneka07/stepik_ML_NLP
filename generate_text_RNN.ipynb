{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b005e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6855b",
   "metadata": {},
   "source": [
    "<font size=6>\n",
    "Preprocess the data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31279de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/author_quotes.txt', 'rt', newline='\\n') as f:\n",
    "    quotes = f.readlines()\n",
    "quotes = [q.rstrip('\\n') for q in quotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = list(set(' '.join(quotes)))\n",
    "char2id = {ch:i for i, ch in enumerate(unique_chars)}\n",
    "pad_value = char2id[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c60808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#char tokenization\n",
    "char_tokens = [[' '] + list(q) for q in quotes]\n",
    "MAX_SENT_LEN = max([len(q) for q in char_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c087f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize\n",
    "vect_data = torch.full((len(char_tokens), MAX_SENT_LEN), char2id[' '], dtype=torch.long)\n",
    "for i, q in enumerate(char_tokens):\n",
    "    for j, ch in enumerate(q):\n",
    "        vect_data[i][j] = char2id[char_tokens[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02b6ce",
   "metadata": {},
   "source": [
    "<font size=6>\n",
    "RNN model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class charRNN(nn.Module):\n",
    "    def __init__(self, char_vocab_len, emb_size=32, num_layers=3, hid_state_size=64):\n",
    "        super().__init__()\n",
    "        self.num_rnn_layers = num_layers\n",
    "        self.hid_state_size = hid_state_size\n",
    "        self.emb = nn.Embedding(char_vocab_len, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hid_state_size, num_layers=num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hid_state_size, char_vocab_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)  # batch_sz x max_sent_len x emb_sz\n",
    "        #print(f\"input emb shape {x.shape}\")\n",
    "        #init h0\n",
    "        hidden = self.get_init_state(x.shape[0])\n",
    "        features, hidden = self.rnn(x, hidden) #features of size batch_sz x max_sent_len x hid_state_size\n",
    "        #print(f\"rnn output shape {features.shape}, {hidden.shape}\")\n",
    "        pred = self.out(features)  #batch_sz x max_sent_len x char_vocab_len\n",
    "        pred = pred.permute(0,2,1)\n",
    "        #pred = pred.argmax(2)\n",
    "        #print(f\"output shape {pred.shape}\")\n",
    "        return pred, hidden\n",
    "    def get_init_state(self, batch_size):\n",
    "        return torch.zeros(self.num_rnn_layers, batch_size, self.hid_state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e603e6",
   "metadata": {},
   "source": [
    "<font size=6>\n",
    "Helper functions\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vectorized_input, lr=0.01, n_epochs=10, batch_size=128, max_num_batches=0):\n",
    "    import time\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #remove the last character from input (nothing to predict for it) and \n",
    "    #the first character from output (since the first char we predict is the second character\n",
    "    #in the sequence)\n",
    "    dataset = torch.utils.data.TensorDataset(vectorized_input[:, :-1],\n",
    "                                             vectorized_input[:, 1:])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        t_beg = time.perf_counter()\n",
    "        mean_loss = 0\n",
    "        nbatches = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            model.zero_grad()\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            print(batch_x[0], batch_y[0])\n",
    "            pred, _ = model(batch_x)\n",
    "            print(f\"pred shape {pred.shape}\")\n",
    "            loss = nn.functional.cross_entropy(pred, batch_y)\n",
    "            mean_loss += float(loss)\n",
    "            nbatches +=1\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if max_num_batches and nbatches == max_num_batches:\n",
    "                break\n",
    "        print(f\"epoch {epoch} loss {mean_loss/nbatches}, epoch time {time.perf_counter()-t_beg}\")\n",
    "    return model\n",
    "\n",
    "def predict_char(model, seed_seq, char2id, id2char):\n",
    "    vect_data = torch.tensor([[char2id[ch] for ch in seed_seq]])\n",
    "    #print(vect_data.shape)\n",
    "    #pass the seed sequence first to get the hidden state\n",
    "    out, _ = model(vect_data)\n",
    "    #print(out.shape)\n",
    "    out_char = out.argmax(1).squeeze(0)\n",
    "    #print(out_char)\n",
    "    return id2char[out_char[-1]] #take the last char from prediction\n",
    "\n",
    "def generate_sequence(model, char2id, id2char, seed_seq = \"hi\", seq_len = MAX_SENT_LEN):\n",
    "    model.eval()\n",
    "    chars = list(seed_seq)\n",
    "    if seq_len <= len(seed_seq):\n",
    "        return seed_seq \n",
    "    \n",
    "    for _ in range(seq_len - len(seed_seq)):\n",
    "        next_char = predict_char(model, chars, char2id, id2char)\n",
    "        chars.append(next_char)\n",
    "    return ''.join(chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17abf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_model = charRNN(len(char2id), num_layers=1)\n",
    "ch_model.load_state_dict(torch.load('./models/RNN-2022-07-05.model', map_location=torch.device('cpu')))\n",
    "#ch_model = train(ch_model, vect_data, lr=0.001, n_epochs=10, max_num_batches=100)\n",
    "#torch.save(ch_model.state_dict(), './models/char_rnn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb40f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(generate_sequence(ch_model, char2id, unique_chars, seq_len=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f12e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv2",
   "language": "python",
   "name": "nlpenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
