{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85caade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918dbaca",
   "metadata": {},
   "source": [
    "***\n",
    "The data file has four news articles, three of them are about the Tesla Roadster car in space and the fourth is about a different topic (a gas company). I want to check the similarity of the four articles using cosine similarity and Eulcidean distance using different vector representations of words. \n",
    "Will follow the article https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630. \n",
    "The expectation is that the first three articles will be assessed as similar, while the fourth one different from them all.\n",
    "\n",
    "A nice article on the interpretation of cosine similarity and Euclidean distance\n",
    "https://www.baeldung.com/cs/euclidean-distance-vs-cosine-similarity\n",
    "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2863d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/roadster_news.csv', header=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab1174",
   "metadata": {},
   "source": [
    "***\n",
    "First, let's compute cosine similarity and ED using the Tf-Idf matrix\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5948dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=nltk.corpus.stopwords.words('russian'), \n",
    "                             token_pattern=r'\\b[^\\d\\W]{4,20}\\b') #\"\\b[a-zA-z]+'?[a-zA-Z]+'\\b\",\n",
    "tfidf_mat = vectorizer.fit_transform(data)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosi = [] #cosine similarity\n",
    "ed = [] #euclidean distance\n",
    "for r1, r2 in itertools.combinations(range(tfidf_mat.shape[0]), 2):\n",
    "    c = np.dot(tfidf_mat[r1], tfidf_mat[r2].T).toarray()[0][0]\n",
    "    d = np.sqrt((tfidf_mat[r2] - tfidf_mat[r1]).power(2).sum())\n",
    "    cosi.append((r1, r2, c))\n",
    "    ed.append((r1, r2, d))\n",
    "#\n",
    "#ed = euclidean_distances(tfidf_mat)\n",
    "#cosine_similarity(tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a546268",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosi.sort(key=lambda v:v[2], reverse=True)\n",
    "ed.sort(key=lambda v:v[2])\n",
    "print(f'Most similar texts are {cosi[0][0]} and {cosi[0][1]} (cosine similarity is {cosi[0][2]} ):')\n",
    "print(f'Least similar texts are {cosi[-1][0]} and {cosi[-1][1]} (cosine similarity is {cosi[-1][2]} ):')\n",
    "print(f'The smallest distance between {ed[0][0]} and {ed[0][1]} (distance is {ed[0][2]} ):')\n",
    "print(f'Biggest distance between texts {ed[-1][0]} and {ed[-1][1]} (distance is {ed[-1][2]} ):')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data[cosi[-1][0]])\n",
    "#print('-')\n",
    "#print(data[cosi[-1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edf334",
   "metadata": {},
   "source": [
    "***\n",
    "Now let's try using GloVe word embeddings. For simplicity, we will consider each document as one sentence and work with doc vectors. Because I use articles written in Russian, I use word embeddings from Navec (https://github.com/natasha/navec#downloads) that were trained using Russian news articles (navec_news_v1_1B_250K_300d_100q.tar). \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe41b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "nv = Navec.load('data/embeddings/navec_news_v1_1B_250K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(vectorizer.build_tokenizer(),data))\n",
    "#min_token_len = 3\n",
    "tokens = [[t.lower() for t in doc_toks if t in vectorizer.vocabulary_] for doc_toks in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430791ca",
   "metadata": {},
   "source": [
    "***\n",
    "Extract from the article:\n",
    "Now we have to represent every document as a single vector. We can either average or sum over every word vector and convert every 64X300 representation into a 300-dimensional representation. But averaging or summing over all the words would lose the semantic and contextual meaning of the documents. Different lengths of the documents would also have an adverse effect on such operations.\n",
    "\n",
    "One better way of doing this could be taking a weighted average of word vectors using the tf-idf weights. This can handle the variable length problem to a certain extent but cannot keep the semantic and contextual meaning of words. After doing that we can use the pairwise distances to calculate similar documents as we did in the tf-idf model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0be5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75848a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up weigted embeddings of words in each document to create a vector representation of the document. \n",
    "emb_sz = nv.pq.dim\n",
    "tfidf_df = pd.DataFrame(tfidf_mat.toarray())\n",
    "docs_emb_glove = np.zeros((len(data), emb_sz))\n",
    "for i in range(len(data)):\n",
    "    for t in tokens[i]:\n",
    "        if t in nv.vocab:\n",
    "            docs_emb_glove[i] += nv[t] * tfidf_df[vectorizer.vocabulary_[t]][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_emb_glove_norm = normalize(docs_emb_glove, axis=1, norm='l2')\n",
    "cosi = [] #cosine similarity\n",
    "ed = [] #euclidean distance\n",
    "for r1, r2 in itertools.combinations(range(docs_emb_glove.shape[0]), 2):\n",
    "    c = np.dot(docs_emb_glove_norm[r1], docs_emb_glove_norm[r2].T)\n",
    "    d = np.sqrt(np.power(docs_emb_glove[r2] - docs_emb_glove[r1], 2).sum())\n",
    "    cosi.append((r1, r2, c))\n",
    "    ed.append((r1, r2, d))\n",
    "#cosine_similarity(docs_emb)\n",
    "#euclidean_distances(docs_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b7d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosi.sort(key=lambda v:v[2], reverse=True)\n",
    "ed.sort(key=lambda v:v[2])\n",
    "print(f'Most similar texts are {cosi[0][0]} and {cosi[0][1]} (cosine similarity is {cosi[0][2]} ):')\n",
    "print(f'Least similar texts are {cosi[-1][0]} and {cosi[-1][1]} (cosine similarity is {cosi[-1][2]} ):')\n",
    "print(f'The smallest distance between {ed[0][0]} and {ed[0][1]} (distance is {ed[0][2]} ):')\n",
    "print(f'Biggest distance between texts {ed[-1][0]} and {ed[-1][1]} (distance is {ed[-1][2]} ):')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc8638",
   "metadata": {},
   "source": [
    "***\n",
    "Now use Word2Vec embeddings. \n",
    "I used CBOW embeddings (news_upos_cbow_300_2_2017.bin.gz) from RusVectores trained on news articles: https://rusvectores.org/ru/models/\n",
    "For preprocessing and POS-tagging I used this script https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_udpipe.py\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e88fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rusvec = gensim.models.KeyedVectors.load_word2vec_format('./data/embeddings/news_0_300_2.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b66133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens = []\n",
    "with open('./data/roadster_news_pos.txt', ) as f:\n",
    "    lines = f.readlines()\n",
    "    pos_tokens = [line.split() for line in lines if len(line) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0e266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fake_tokenizer(text):\n",
    "    return text\n",
    "#build tfidf matrix for tagged and lemmatized tokens\n",
    "vectorizer_rusvec = TfidfVectorizer(tokenizer=fake_tokenizer, lowercase=False)\n",
    "tfidf_mat_rusvec = vectorizer_rusvec.fit_transform(pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c35f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up weigted embeddings of words in each document to create a vector representation of the document. \n",
    "emb_sz = rusvec.vector_size\n",
    "docs_emb_w2v = np.zeros((len(data), emb_sz))\n",
    "tfidf_rusvec_df = pd.DataFrame(tfidf_mat_rusvec.toarray())\n",
    "for i in range(len(pos_tokens)):\n",
    "    for t in pos_tokens[i]:\n",
    "        if t in rusvec:\n",
    "            docs_emb_w2v[i] += rusvec.get_vector(t) * tfidf_rusvec_df[vectorizer_rusvec.vocabulary_[t]][i]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b557916",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_emb_w2v_norm = normalize(docs_emb_w2v, axis=1, norm='l2')\n",
    "cosi = [] #cosine similarity\n",
    "ed = [] #euclidean distance\n",
    "for r1, r2 in itertools.combinations(range(docs_emb_w2v.shape[0]), 2):\n",
    "    c = np.dot(docs_emb_w2v_norm[r1], docs_emb_w2v_norm[r2].T)\n",
    "    d = np.sqrt(np.power(docs_emb_w2v[r2] - docs_emb_w2v[r1], 2).sum())\n",
    "    cosi.append((r1, r2, c))\n",
    "    ed.append((r1, r2, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosi.sort(key=lambda v:v[2], reverse=True)\n",
    "ed.sort(key=lambda v:v[2])\n",
    "print(f'Most similar texts are {cosi[0][0]} and {cosi[0][1]} (cosine similarity is {cosi[0][2]} ):')\n",
    "print(f'Least similar texts are {cosi[-1][0]} and {cosi[-1][1]} (cosine similarity is {cosi[-1][2]} ):')\n",
    "print(f'The smallest distance between {ed[0][0]} and {ed[0][1]} (distance is {ed[0][2]} ):')\n",
    "print(f'Biggest distance between texts {ed[-1][0]} and {ed[-1][1]} (distance is {ed[-1][2]} ):')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68550a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(docs_emb_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances(docs_emb_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd888c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv2",
   "language": "python",
   "name": "nlpenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
